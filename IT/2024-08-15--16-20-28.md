nn.functional.log_softmax(/exp/log_softmax 相比 softmax，对较小的概率分布处理能力更好。 例如向量 x=(−999,1,2,5,6) 𝑥 = ( − 999 , 1 , 2 , 5 , 6 ) ，使用softmax处理后，得到的概率分布如下： (0.0000,0.0048,0.0131,0.2641,0.7179) ( 0.0000 , 0.0048 , 0.0131 , 0.2641 , 0.7179 ) 使用 log_softmax 之后得到的概率分布如下： (−1.0053e+03,−5.3314e+00,−4.3314e+00,−1.3314e+00,−3.3141e−01) ( − 1.0053 𝑒 + 03 , − 5.3314 𝑒 + 00 , − 4.3314 𝑒 + 00 , − 1.3314 𝑒 + 00 , − 3.3141 𝑒 − 01 ) 可见使用 softmax ，第一个数直接变0，当经过一次迭代 x=(−990,1.5,2.1,5.6,6.8) 𝑥 = ( − 990 , 1.5 , 2.1 , 5.6 , 6.8 ) 之后，使用 softmax 得到的概率分布第一个元素仍为0，因此求得梯度为0，这个数据就不能用于更新参数，而使用 log_softmax  可解决此问题在实际应用中，log_softmax通常用于计算损失函数，而softmax则用于预测